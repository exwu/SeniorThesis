\documentclass{article}

\usepackage[margin=0.8in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{natbib}
\usepackage{bbm}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{calc,positioning,shapes,arrows}
%\usepackage{natbib}
%\bibliographystyle{unsrtnat}
\newcommand{\Iota}{\mathcal{I}}

\title{Social Feedback for Robotic Collaboration}
\author{Emily Wu, Brown University}
\begin{document}
\maketitle

\tableofcontents

\newpage
\section{Abstract}
\newpage

\section{Introduction}

Collaboration is a process that relies heavily on communication for its success. When humans collaborate, this communication is both obvious and implicit---we not only instruct and request aid from each other, but we also tacitly monitor our partners for signs of approval and understanding while producing these signals ourselves. However, in robotics, many of these vital components of successful communication are missing or lost. The absence of these signals likely account for numerous failures in human-robot collaborative tasks. In order to give robots these missing communicative skills, we employ \emph{social feedback} signals to provide human-like communication to both inform the human participant of the robot's state and request information from the participant. 

Feedback refers to the responses that are received by an agent when it takes some action. These feedback responses inform the agent about the success or failure of its actions. \emph{Social feedback} therefore refers to social signals that convey this information. These can be explicit signals, such as ``I want this object'', or implicit signals, like a perplexed expression. When humans interact with another human, they use this feedback to improve the flow and clarity of the collaboration, which in turn improves its success. For my thesis work, I have developed a framework that describes how robotic agent should likewise generate social feedback for its human partner in human-collaborative tasks, and show that it improves the speed and accuracy of human-robot interaction. 

In this work, we address the object delivery task. In this task, a set of objects are laid out on a table within the robot's reach. A human participant requests an object from the robot using speech and gesture (pointing). The robot must interpret the human's speech and gesture and deliver the requested object to the human. The task then repeats with the remaining objects on the table. This task is achievable without social feedback; the robot need only wait until enough information is given and then deliver the correct object. However, we will show that by adding social feedback actions, such as asking questions, looking at objects, and pointing at objects, we will achieve better accuracy and speed as well as improved user experience. 

We will solve this problem by formulating it as a Partially Observable Markov Decision Process (POMDP), which will allow us to dynamically and flexibly determine how to choose social feedback actions. First, we will describe a two agent model and use this to motivate the construction of a POMDP. We will next discuss how we solved this POMDP and other measures we took to allow our system to respond dynamically and fluidly. 

To evaluate our approach, Stop. Hammertime. 


\section{Related Work}

\section{Technical Approach}

\subsection{POMDP Overview}

Partially Observable Markov Decision Processes (POMDPs)\textbf{citation} are used to model Markov Decision Processes\textbf{citation}where the true state is not known. Instead, the agent receives observations that are generated by the true state, and must infer what the true state is from the observations. Thus, the agent maintains a belief over true states which is updated as it receives new observations. The agent must then use this belief state to determine which action to take to maximize its expected value of the reward over time. This splits the POMDP into two main components, a state estimator and a policy generator. 

\paragraph{State Estimator}

The state estimator is given an initial belief and uses Bayesian mathematics to update its belief over time. In order to perform this update, the state estimator has a model of how the hidden states emit observations as well as how the state evolves in response to actions performed by the robot. The first model is described by the observation function and the second by the transition function. 

\paragraph{Policy Generator}

The policy generator is the means by which actions are chosen by the agent. Specifically, the policy generator should choose actions that maximize the agent's expected reward over time. The reward for taking a particular action in a state is given by a reward function. These state-action pairs are called policies, and are generated by a POMDP Solver. Generally, POMDPs are solved by converting to their equivalent belief-MDP, which is an MDP where the state is distribution over the POMDP's hidden states, a belief. In subsequent sections we will refer to this belief over states as $b$. 


Formally, a POMDP is defined as a tuple $\{S, A, T, R, \Omega, O\}$, where : 
\begin{itemize}
	\item $s \in S$ is a hidden state. 
	\item $a \in A$ is an action that an action takes. 
	\item $T: S \times A \times S \to \mathcal{R}$ is a transition function that returns returns the value $p(s^\prime | s, a)$, the probability of transitioning to state $s^\prime \in  S$ from state $s \in S$ when taking action $a \in A$. 
	\item $R: S \times A \to \mathcal{R}$ is a reward function that returns the reward for taking action $a \in A$ in state $s \in S$. 
	\item $o \in \Omega$ is an observation. 
	\item $O: S \times A \times O \to \mathcal{R}$ is an observation function that returns the value of $p(o | s, a)$, the probability of observing observation $o \in O$ when action $a \in A$ transitions into $s \in S$. 
\end{itemize}

A typical bayesian network for a POMDP is as follows. 

\tikzstyle{h} = [circle, draw, fill=gray!40, minimum size=4em]
\tikzstyle{a} = [diamond, draw, fill=gray!40, minimum size=4em]
\tikzstyle{v} = [circle, draw, fill=white, minimum size=4em]
\tikzstyle{line} = [draw, > = stealth, -latex]


\begin{center}
	\begin{tikzpicture}[node distance = 2.5cm, auto]
		%actions
		\node [v] (ort) {$o_{t}$};
		\node [a, right of=ort] (oht) {$a_{t}$};
		\node [v, right of=oht] (ortp1) {$o_{t+1}$}; 
		\node [a, left of=ort] (ohtm1) {$a_{t-1}$};
		\node [v, left of=ohtm1] (ortm1) {$o_{t-1}$};
		% states
		\node [h, above of=ort] (st) {$s_t$}; 
		\node [h, above of=ortm1] (stm1) {$s_{t-1}$}; 
		\node [h, above of=ortp1] (stp1) {$s_{t+1}$}; 
		% edges; state -> obs
		\path [line] (st) edge (ort); 
		\path [line] (stm1) edge (ortm1); 
		\path [line] (stp1) edge (ortp1); 
		% state -> state
		\path [line] (stm1) edge (st); 
		\path [line] (st) edge (stp1); 
		% a -> state
		\path [line] (ohtm1) edge (st); 
		\path [line] (oht) edge (stp1); 
	\end{tikzpicture}
\end{center}


\subsection{Model Description}

To model this human-robot interactive task, we will use a POMDP. To motivate the construction of our model, consider the following two-agent Bayesian network. 


\tikzstyle{h} = [circle, draw, fill=gray!40, minimum size=4em]
\tikzstyle{v} = [circle, draw, fill=white, minimum size=4em]
\tikzstyle{line} = [draw, > = stealth, -latex]
\begin{center}
	\begin{tikzpicture}[node distance = 2.5cm, auto]
		%actions
		\node [v] (ort) {$o_{H,t}$};
		\node [v, right of=ort] (oht) {$o_{R, \tau}$};
		\node [v, right of=oht] (ortp1) {$o_{H,t+1}$}; 
		\node [v, right of=ortp1] (ohtp1) {$o_{R,\tau+1}$}; 
		\node [v, left of=ort] (ohtm1) {$o_{R, \tau-1}$};
		\node [v, left of=ohtm1] (ortm1) {$o_{H, t-1}$};
		% states
		\node [h, above of=ort] (st) {$s_t$}; 
		\node [h, below of=oht] (sigmat) {$\sigma_\tau$};
		\node [h, above of=ortm1] (stm1) {$s_{t-1}$}; 
		\node [h, below of=ohtm1] (sigmatm1) {$\sigma_{\tau-1}$};
		\node [h, above of=ortp1] (stp1) {$s_{t+1}$}; 
		\node [h, below of=ohtp1] (sigmatp1) {$\sigma_{\tau+1}$};
		% edges; state -> obs
		\path [line] (st) edge (ort); 
		\path [line] (sigmat) edge (oht); 
		\path [line] (stm1) edge (ortm1); 
		\path [line] (sigmatm1) edge (ohtm1); 
		\path [line] (stp1) edge (ortp1); 
		\path [line] (sigmatp1) edge (ohtp1); 
		% state -> state
		\path [line] (stm1) edge (st); 
		\path [line] (st) edge (stp1); 
		\path [line] (sigmatm1) edge (sigmat); 
		\path [line] (sigmat) edge (sigmatp1); 
		% a -> state
		\path [line] (ortm1) edge (sigmatm1); 
		\path [line] (ort) edge (sigmat); 
		\path [line] (ortp1) edge (sigmatp1); 
		\path [line] (ohtm1) edge (st); 
		\path [line] (oht) edge (stp1); 
	\end{tikzpicture}
\end{center}

In the above model, we represent human states $s_t$, robot states $\sigma_\tau$, and observations generated by the robot $o_{R,t}$ and by the human $o_{H,t}$. Observe that the structure of this model resembles two POMDPs combined together at their actions and observations. The lower POMDP is the POMDP from the human's perspective: the robot has some hidden states $s_t$, which the human observes by means of $o_{R,t}$. The human takes actions $o_{H, \tau - 1}$ to influence the robot's state $s_t$. The upper POMDP is likewise the a POMDP that models the robot's process: the human's state $\sigma_\tau$ is hidden from the robot, and the robot must infer it from observations $o_{H, t}$. When the robot takes action $o_{R, t}$, it affects the human's state $\sigma_\tau$. Crucially, each agent treats the other agent's as an action that influences their belief about the other agent's hidden state. Thus, the human's actions affect the robot's belief about the human's state, which is what we call $b$ as defined above. Importantly, the reverse is also true: the robot's actions affect the human's belief about what the robot's hidden state is. We will call the human's belief over the robot's hidden state $\beta$. 


In the following section we will use this dual structure to inform the construction of our POMDP as applied to our object delivery domain. 


\subsection{POMDP Definition}

We define our object-delivery POMDP as a tuple $\{S, A, T, R, \Omega, O\}$: 

\begin{itemize}
	\item Each $s \in S$ is a tuple of $\langle \iota, \beta, \Iota \rangle$		\begin{itemize}
			\item $\Iota$ is the set of all objects that the robot can deliver. Each object is parameterized by a name, a unigram vocabulary, and a position; for example: a red bowl would be represented $\langle$redBowl, [red, red, bowl, bowl, plastic], (1.0, 2.0, 0.0)$\rangle$. We assume the set of all objects is known. 
			\item $\iota \in \Iota$ is the object the human desires. This is a hidden variable. 
			\item $\beta$ is a distribution over the robot's hidden states, as defined above. In this domain, the robot state $\sigma$ represents which object the robot will hand the human, or which object the robot believes the human wants. 
		\end{itemize}
	\item The set of actions A consists of both social and non-social actions. Non-social actions are \texttt{pick(i)} (picking up  and delivering an object $i$) and \texttt{wait}. Social actions are \texttt{point(x)}, pointing at a location $x$; \texttt{look(x)}, looking at a location $x$, \texttt{say(p)}, informing the human that the robot believes the desired object has property $p$. $p$ is an element of some object's vocabulary. 
	\item $T(s, a, s^\prime) = p(s^\prime | s, a)$. We make the assumption that the human's desired object $\iota$ does not change unless the robot picks up object $\iota$. The set of all objects $\Iota$ transistions to $\Iota \setminus \{i\}$ when the robot chooses action \texttt{pick(i)}. For each action in $A$, we define a ``reverse observation function'' that describes our assumptions about how the human imagines the robot generates actions (which the human sees as observations) given a hidden robot state $\sigma$. This is gone into further detail in section  \ref{sssec:tf}. 
	\item $R(s, a)$ takes as input a state $s \in S$ and an action $a \in A$. In this domain, we incentivize our robot to pick the correct object by giving it a +10 reward if it delivers the correct object and a -50 reward for picking the incorrect object. We also give negative rewards for taking varioius actions: \texttt{wait} receives a -1 reward (to incentivize the robot to finish the task quickly); \texttt{look(x)} receives a reward of -2; \texttt{say(p)} receives a reward of -3; \texttt{point(x)}  receives a reward of -4. These additional penalties for social actions reflect the penalty for ``bothering the user'', as well as the time it takes to execute these actions. 
	\item Observation $o \in \Omega$ represents an observation generated by the human. These are tuples of language and gesture: $\langle l, g \rangle$. Language utterance $l$ is represented by a string of any number of words, obtained by transcribing microphone input using webkit's speech recognition API.  A gesture $g$ is represented by a vector from the participant's shoulder to their wrist, and all gestures are interpreted as a straight-armed point. This vector is obtained using the Kinect's tracking software. 
	\item $O(o, s, a) = p(o |s, a)$ describes the probability of of seeing an observation $o$ from the human given their state $s$ and the robot's last action $a$ (though we do not make use of this last parameter). We choose an observation function that reflects that the human is an agent attempting to communicate which object they desire to the human, and thus chooses to generate observations that are more likely to result in the robot delivering the correct object. This is gone into more detail in section \ref{sssec:of}
\end{itemize}


\subsubsection{Observation Function} \label{sssec:of}

According to our double-agent model, the human emits observations as though it were an agent interacting with our robotic agent. Thus, we choose an observation model that depends on the human's belief about the robot's state, $\beta$. Specifically, the human will choose an action according to the its estimate that the robot will hand them that object. In order to define this function, we will first have to define a base-level observation function. 

\paragraph{Base-Level Observation Function}

The base level observation function describes the probability of an observation conditioned only on the object: $p(o|\iota)$. For our object delivery domain, we will define two base-level observations, one for language and one for gesture. 

\noindent\textit{Speech Model:} Language is interpreted according to a unigram gesture model. An utterance $l$ is broken down into individual words, $w \in l$: 

\begin{equation}
	p(l|\iota) = \prod_{w \in l} p(w|\iota) = \prod_{w\in l} \frac{\texttt{count}(w, \iota.\texttt{vocab})}{|\iota.\texttt{vocab}|}
\end{equation}

where $\texttt{count}(w, \iota.\texttt{vocab})$ is the number of times word $w$ appears in $\iota$'s vocabulary. 

\noindent\textit{Gesture Model:} All gestures are interpreted as a straight armed point. These pointing gestures are selected from a normal distribution centered at the object's location. 

Define the angle between the vector defined by the pointing gesture and the vector from the human's arm to the object $\iota$ to be $\theta_\iota$. The probability of a particular gesture is then

\begin{equation}
p(g|\iota) = \mathcal{N}(\theta_\iota | 0, v)
\end{equation}

where $v$ is a hand-tuned variance. 

\paragraph{Posterior Observation Function}

We will use the base-level observation functions defined above to define a posterior observation that considers the effects of the base level observation function. Specifically, the human chooses an observation proportional to the robot's belief in the desired object if the human had chosen that observation, i.e., 

\begin{align}
p(o|s) = p(o|\iota, \beta) &\triangleq \eta p_\beta(\iota|o) \\
&= \eta \frac{p(o|\iota) p(\iota)}{\sum_{\iota^\prime} p(o|\iota^\prime)p(\iota^\prime)} 
\end{align}

Next, we will use we will set $p(\iota) = \beta(\iota)$. $p(\iota)$ describes the robot's belief in $\iota$. This value is represented in the robot's belief-state vector, $b$. But since the human does not know $b$, it uses its estimate of $b$, $\beta$. Our new expression is 


$$p(o|s) = \eta \frac{p(o|\iota) \beta(\iota)}{\sum_{\iota^\prime} p(o|\iota^\prime)\beta(\iota^\prime)} $$


\paragraph{Toy Example} 

In order to demonstrate and motivate this observation function, we will define a toy domain. In this toy domain, the states are the following: $\{AA, AB, BA, BB\}$ as well as $\beta$, which is a distribution over these states. The observations are $\{A\_, \_A, B\_, \_B\}$, which mean ``the first character is an A'', ``the second character is an A'', ``the first character is B'', ``and the second character is a B'' respectively. These observations are provided by the human. The agent can take actions $CX$ which is informing the human that the agent believes the Xth character is a C, as well as ``picking'' the object or waiting. In our toy domain, for the base leve observation, the human always gives truthful observations, and has equal probability of generating an observation pertaining to a particular character. We will see how this affects the POMDP observation function, which incorporates $\beta$. 

Consider the following situation: 

Intially both $b$, the robot's belief about the human's desired object and $\beta$, the human's belief about the robot's belief, are uniform. The true state is $AA$. 

\begin{center}
\begin{tabular}{|c| c| c| c | c|}
	\hline
	$t$ & $b$ & $\beta$ & $a$ & $o$ \\
	\hline
	$0$ & [0.25, 0.25, 0.25, 0.25] & [0.25, 0.25, 0.25, 0.25] & & \\
	\hline
\end{tabular}
\end{center}


The robot then receives an observation $A\_$. The new beliefs are: 

\begin{center}
\begin{tabular}{|c| c| c| c | c|}
	\hline
	$t$ & $b$ & $\beta$ & $a$ & $o$ \\
	\hline
	$0$ & [0.25, 0.25, 0.25, 0.25] & [0.25, 0.25, 0.25, 0.25] & & \\
	\hline
	$1$ & [0.50, 0.50, 0, 0] & [0.25, 0.25, 0.25, 0.25] & & $A\_$ \\
	\hline
\end{tabular}
\end{center}

Next, the robot can choose to take an action. If it chooses to wait, this is the resulting state


\begin{center}
\begin{tabular}{|c| c| c| c | c|}
	\hline
	$t$ & $b$ & $\beta$ & $a$ & $o$ \\
	\hline
	$0$ & [0.25, 0.25, 0.25, 0.25] & [0.25, 0.25, 0.25, 0.25] & & \\
	\hline
	$1$ & [0.50, 0.50, 0, 0] & [0.25, 0.25, 0.25, 0.25] & & $A\_$ \\
	\hline
	$2$ & [0.50, 0.50, 0, 0] & [0.25, 0.25, 0.25, 0.25] & wait  &  \\
	\hline
\end{tabular}
\end{center}

Examine the probabilities of each observation.

\begin{align*}
	p(A\_ | AA, \beta) &= \frac{ p(A\_)\beta(AA) }{ p(A\_)\beta(AA) + p(A\_)\beta(AB) + p(A\_)\beta(BA) + p(A\_)\beta(BB)} \\
	&= \frac{0.5 * 0.25}{0.5 * 0.25 + 0.5 * 0.25 + 0 + 0} \\
	&= 0.5
\end{align*}

\begin{align*}
	p(\_A | AA, \beta) &= \frac{ p(\_A)\beta(AA) }{ p(\_A)\beta(AA) + p(\_A)\beta(AB) + p(\_A)\beta(BA) + p(\_A)\beta(BB)} \\
	&= \frac{0.5 * 0.25}{0.5 * 0.25 + 0 + 0.5 * 0.25 + 0} \\
	&= 0.5
\end{align*}

The probabilities of all other actions are 0, since we only give truthful observations.

Notice that this situation is unideal. The agent has equal probabilities of receiving either observation, even though the agent already knows that the first character is an A. Now, consider what would happen if the robot chose the action $A0$. We would get the following belief states: 


\begin{center}
\begin{tabular}{|c| c| c| c | c|}
	\hline
	$t$ & $b$ & $\beta$ & $a$ & $o$ \\
	\hline
	$0$ & [0.25, 0.25, 0.25, 0.25] & [0.25, 0.25, 0.25, 0.25] & & \\
	\hline
	$1$ & [0.50, 0.50, 0, 0] & [0.25, 0.25, 0.25, 0.25] & & $A\_$ \\
	\hline
	$2$ & [0.50, 0.50, 0, 0] & [0.5, 0.5, 0, 0] & $A0$  &  \\
	\hline
\end{tabular}
\end{center}

If we examine the probabilities again: 

\begin{align*}
	p(A\_|AA, \beta) &\propto \frac{ p(A\_)\beta(AA) }{ p(A\_)\beta(AA) + p(A\_)\beta(AB) + p(A\_)\beta(BA) + p(A\_)\beta(BB)} \\
	&\propto \frac{0.5 * 0.5}{0.5 * 0.5 + 0.5 * 0.5 + 0 + 0} \\
	&\propto 0.5 \\
	&= \frac{1}{3}
\end{align*}

\begin{align*}
	p(\_A|AA, \beta) &\propto \frac{ p(\_A)\beta(AA) }{ p(\_A)\beta(AA) + p(\_A)\beta(AB) + p(\_A)\beta(BA) + p(\_A)\beta(BB)} \\
	&\propto \frac{0.5 * 0.5}{0.5 * 0.5 + 0 + 0 + 0} \\
	&\propto 1 \\
	&= \frac{2}{3}
\end{align*}
 
Now, we are more likely to get the observation $\_A$, which is more useful to us than the observation $A\_$, since it gives us the information we need to pick the correct object, $AA$. 

\paragraph{Modification for Object Delivery Domain}


In the object delivery domain, we have both speech and gesture, which we assume are conditionally independent given the state. 

$$p(o|s) = p(l|s)p(g|s)$$

In addition, it is possible to observe no speech or no gesture input. Let $L$ be a random variable that is 1 if the agent receives a language observation and 0 otherwise. Similarly, let $G$ be a random variable that is 1 if the agent receives a gesture observation and 1 otherwise. For all states $s$: 

\begin{align*}
	p(L = 1 | s) &= c_L \\
	p(L = 0 | s) &= 1 - c_L 
\end{align*}

\begin{align*}
	p(G = 1 | s) &= c_G \\
	p(G = 0 | s) &= 1 - c_G 
\end{align*}

where $c_L$ and $c_G$ are constant values less than 1. 

For each of language and gesture, we will use their own posterior observation function. Combined with the probability to receive a null speech or null gesture observation, the full expression is as follows: 



$$p(o|s) = \eta p(l|s) p(g|s) = \eta p(L|s) \left[ \frac{p(l|\iota) \beta(\iota)}{\sum_{\iota^\prime} p(l|\iota^\prime)\beta(\iota^\prime)} \right] ^ L \cdot p(G|s) \left[ \frac{p(g|\iota) \beta(\iota)}{\sum_{\iota^\prime} p(g|\iota^\prime)\beta(\iota^\prime)} \right]^G$$




\subsubsection{Transition Function} \label{sssec:tf}

We make the assumption that every component of our state transitions conditionally independent of the other components given the previous state. In addition, each state variable depends only on its previous value and the action taken. 

\begin{align*}
	p(\iota_{t+1}, \beta_{t+1}, \Iota_{t+1} | \iota_t, \beta_t, \Iota_t, a_t)  =  p(\iota_{t+1} | \iota_t, a_t) p(\beta_{t+1} | \beta_t, a) p(\Iota_{t+1} | \Iota_t, a_t)
\end{align*}

\paragraph{Object Transition Function}
As previously stated, we assume the object the human desires does not change unless its desired object is picked up. If the desired object is picked, it transitions uniformly at random between the remaining objects. 

If $a_t$ is \texttt{pick($\iota_t$)}, 

\begin{equation}
	p(\iota_{t+1} | \iota_t, a_t = \texttt{pick($\iota_t$)}) = \begin{cases}
		1/|\Iota| & \text{if } \iota_{t+1} \in \Iota \text{ and } \iota_{t+1} != \iota \\
		0 & \text{otherwise} 
	\end{cases}
\end{equation}


If $a_t$ is any other action: 

\begin{equation}
	p(\iota_{t+1} | \iota_t, a_t) = \begin{cases} 
		1 & \text{if $\iota_{t+1} = \iota_t$} \\
		0 & \text{otherwise}
	\end{cases}
\end{equation}

\paragraph{Object Set Transition Function}

\paragraph{Belief Transition Function}


\subsection{Policy Generation}


\subsection{Incremental System Design}

Interactive systems that involve speech often make an assumption that the interaction can be split neatly into turns: first one speaker provides a complete utterance, and then the other. This is often not the case, and it is a subtle matter to determine when one turn is completed and the other agent can begin to speak. As a consequence, many systems involving dialogue, including earlier iterations of this project, do not ``hear'' utterances or gestures while the agent is speaking or acting. Any speech or gestures the human made while the robot was executing its action were dropped and never incorporated into the robot's belief about the state. An additional problem was that while the robot was planning its next action, it would also not incorporate any observations made. This was a significant affect, as it could take up to 10 seconds of planning for the robot to make its next move. As a consequence, most observations would be dropped unless we incorporated a system of turn-taking where we waiting a set amount of time for the human to provide any speech and gesture input. 

While turn-taking is an adequate solution for dialogue-only systems, full fledged interaction is more continuous. Even when speaking in a turn-like manner, agents give signals back and forward to signal their understanding and also provide corrections and interruptions.

In order to create more fluid interactions, we employ a multithreaded solution that separates the state estimation, planning, and executions of actions. This allows our robot to simultaneously track the human's belief, plan its next move, and interact with the human. 


\tikzstyle{block} = [rectangle, draw, fill=blue!20, text width=8em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, > = stealth, -latex]


\begin{center}
\begin{tikzpicture}[auto]
	\node [block] (st) {State Tracker}; 
	\node [left = 2.5cm of st] (n) {};
	\node [block, right = 2.5cm of st] (ap1) {Asynchronous Planner}; 
	\node [block, above = 0.5cm of ap1] (ap2) {Asynchronous Planner}; 
	\node [block, below = 0.5cm of ap1] (ap3) {Asynchronous Planner};
	\node [block, right = 2.5cm of ap1] (aq) {Action Queue};
	\path [line] (st) -- node[above] {belief state} (ap1); 
	\path [line] (st) |- node[above right, pos=0.72] {belief state} (ap2); 
	\path [line] (st) |- node[above right, pos=0.72] {belief state} (ap3); 
	\path [line] (ap1) -- node[above] {action} (aq); 
	\path [line] (ap2) -| node[above left, pos=0.22] {acton} (aq); 
	\path [line] (ap3) -| node[above left, pos=0.22] {acton} (aq); 
	\path [line] (n) -- node[above]{observations} (st);
\end{tikzpicture}
\end{center}




\paragraph{State Tracker} The state tracker incorporates observations from the human at a very high rate, allowing the tracker to always have access to the most up-to-date state estimation. However, our domain description requires the robot to choose an action between each consecutive observation. We address this by feeding the model a \texttt{wait} action unless the planning thread has provided a action that has not yet been integrated, in which case we use that action. This also requires minimal dependency between actions and observations, which our model avoids. 

\paragraph{Asynchronous Planner} A separate thread runs the planning code. Planning can take several seconds (though less is ideal), during which the robot is not only idle, but also deaf and blind. Running the planning code on a separate thread allows the robot to continuing observing and interacting with the robot while the robot plans its next move. In addition, we can use several planning threads at once, with separate policies to control the robot's face, arms, head angle, etc. In our work, we use one thread to plan for the POMDP domain described above, and another to animate the Baxter's face to show an facial expression of increasing confusion with the entropy of the belief state. 

\paragraph{Action Queue} The planning threads push their actions onto a queue which executes them one by one, allowing the planning threads to resume planning if the actions they produce take some time to execute. An additional advantage is that this allows the robot to be constantly acting, with minimal idle time. 

\section{Evaluation}

\subsection{User Studies}

\section{Future Work}



\end{document}
